
@misc{noauthor_emote-controlled_nodate,
	title = {Emote-{Controlled}: {Obtaining} {Implicit} {Viewer} {Feedback} {Through} {Emote}-{Based} {Sentiment} {Analysis} on {Comments} of {Popular} {Twitch}.tv {Channels}: {ACM} {Transactions} on {Social} {Computing}: {Vol} 3, {No} 2},
	url = {https://dl.acm.org/doi/abs/10.1145/3365523},
	urldate = {2023-04-26},
	file = {3365523:/Users/giovanna/Zotero/storage/2QPXTCZZ/3365523.html:text/html},
}

@article{kobs_emote-controlled_2020,
	title = {Emote-{Controlled}: {Obtaining} {Implicit} {Viewer} {Feedback} {Through} {Emote}-{Based} {Sentiment} {Analysis} on {Comments} of {Popular} {Twitch}.tv {Channels}},
	volume = {3},
	shorttitle = {Emote-{Controlled}},
	doi = {10.1145/3365523},
	abstract = {In recent years, streaming platforms for video games have seen increasingly large interest, as so-called esports have developed into a lucrative branch of business. Like for other sports, watching esports has become a new kind of entertainment medium, which is possible due to platforms that allow gamers to live stream their gameplay, the most popular platform being Twitch.tv. On these platforms, users can comment on streams in real time and thereby express their opinion about the events in the stream. Due to the popularity of Twitch.tv, this can be a valuable source of feedback for streamers aiming to improve their reception in a gaming-oriented audience. In this work, we explore the possibility of deriving feedback for video streams on Twitch.tv by analyzing the sentiment of live text comments made by stream viewers in highly active channels. Automatic sentiment analysis on these comments is a challenging task, as one can compare the language used in Twitch.tv with that used by an audience in a stadium, shouting as loud as possible in sometimes nonorganized ways. This language is very different from common English, mixing Internet slang and gaming-related language with abbreviations, intentional and unintentional grammatical and orthographic mistakes, and emoji-like images called emotes . Classic lexicon-based sentiment analysis techniques therefore fail when applied to Twitch comments.
To overcome the challenge posed by the nonstandard language, we propose two unsupervised lexicon-based approaches that make heavy use of the information encoded in emotes, as well as a weakly supervised neural network–based classifier trained on the lexicon-based outputs, which is supposed to help generalization to unknown words by use of domain-specific word embeddings. To enable better understanding of Twitch.tv comments, we analyze a large dataset of comments, uncovering specific properties of their language, and provide a smaller set of comments labeled with sentiment information by crowdsourcing.
We present two case studies showing the effectiveness of our methods in generating sentiment trajectories for events live streamed on Twitch.tv that correlate well with specific topics in the given stream. This allows for a new kind of implicit real-time feedback gathering for Twitch streamers and companies producing games or streaming content on Twitch.
We make our datasets and code publicly available for further research.},
	journal = {ACM Transactions on Social Computing},
	author = {Kobs, Konstantin and Zehe, Albin and Bernstetter, Armin and Chibane, Julian and Pfister, Jan and Tritscher, Julian and Hotho, Andreas},
	month = may,
	year = {2020},
	pages = {1--34},
	file = {Full Text PDF:/Users/giovanna/Zotero/storage/9VNRFKKQ/Kobs et al. - 2020 - Emote-Controlled Obtaining Implicit Viewer Feedba.pdf:application/pdf},
}

@article{kim_understanding_2022,
	title = {Understanding and identifying the use of emotes in toxic chat on {Twitch}},
	volume = {27},
	issn = {2468-6964},
	url = {https://www.sciencedirect.com/science/article/pii/S2468696421000598},
	doi = {10.1016/j.osnem.2021.100180},
	abstract = {The latest advances in NLP (natural language processing) have led to the launch of the much needed machine-driven toxic chat detection. Nevertheless, people continuously find new forms of hateful expressions that are easily identified by humans, but not by machines. One such common expression is the mix of text and emotes, a type of visual toxic chat that is increasingly used to evade algorithmic moderation and a trend that is an under-studied aspect of the problem of online toxicity. This research analyzes chat conversations from the popular streaming platform Twitch to understand the varied types of visual toxic chat. Emotes were sometimes used to replace a letter, seek attention, or for emotional expression. We created a labeled dataset that contains 29,721 cases of emotes replacing letters. Based on the dataset, we built a neural network classifier and identified visual toxic chat that would otherwise be undetected through traditional methods and caught an additional 1.3\% examples of toxic chat out of 15 million chat utterances.},
	language = {en},
	urldate = {2023-04-26},
	journal = {Online Social Networks and Media},
	author = {Kim, Jaeheon and Wohn, Donghee Yvette and Cha, Meeyoung},
	month = jan,
	year = {2022},
	keywords = {Algorithmic moderation, Detection, Emotes, Live streaming, Twitch, Usages, Visual toxic chat},
	pages = {100180},
	file = {ScienceDirect Full Text PDF:/Users/giovanna/Zotero/storage/PBW28IMF/Kim et al. - 2022 - Understanding and identifying the use of emotes in.pdf:application/pdf;ScienceDirect Snapshot:/Users/giovanna/Zotero/storage/Y3UH39FK/S2468696421000598.html:text/html},
}

@misc{noauthor_twitch_2023,
	title = {Twitch (service)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Twitch_(service)&oldid=1151534465},
	abstract = {Twitch is an American video live streaming service that focuses on video game live streaming, including broadcasts of esports competitions, in addition to offering music broadcasts, creative content, and "in real life" streams. Twitch is operated by Twitch Interactive, a subsidiary of Amazon.com, Inc. It was introduced in June 2011 as a spin-off of the general-interest streaming platform Justin.tv. Content on the site can be viewed either live or via video on demand. The games shown on Twitch's current homepage are listed according to audience preference and include genres such as real-time strategy games (RTS), fighting games, racing games, and first-person shooters.The popularity of Twitch eclipsed that of its general-interest counterpart. In October 2013, the website had 45 million unique viewers, and by February 2014, it was considered the fourth-largest source of peak Internet traffic in the United States. At the same time, Justin.tv's parent company was re-branded as Twitch Interactive to represent the shift in focus – Justin.tv was shut down in August 2014. That month, the service was acquired by Amazon for US\$970 million, which later led to the introduction of synergies with the company's subscription service Amazon Prime. Twitch acquired Curse LLC in 2016, an operator of online video gaming communities and introduced means to purchase games through links on streams along with a program allowing streamers to receive commissions on the sales of games that they play.
By 2015, Twitch had more than 100 million viewers per month. In 2017, Twitch remained the leading live streaming video service for video games in the US, and had an advantage over YouTube Gaming, which shut down its standalone app in May 2019. As of February 2020, it had 3 million broadcasters monthly and 15 million daily active users, with 1.4 million average concurrent users. As of May 2018, Twitch had over 27,000 partner channels.},
	language = {en},
	urldate = {2023-04-26},
	journal = {Wikipedia},
	month = apr,
	year = {2023},
	note = {Page Version ID: 1151534465},
	file = {Snapshot:/Users/giovanna/Zotero/storage/EZZ4ECJI/Twitch_(service).html:text/html},
}

@misc{noauthor_long_2023,
	title = {Long short-term memory},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Long_short-term_memory&oldid=1148032239},
	abstract = {Long short-term memory (LSTM) is an artificial neural network used in the fields of artificial intelligence and deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. Such a recurrent neural network (RNN) can process not only single data points (such as images), but also entire sequences of data (such as speech or video). This characteristic makes LSTM networks ideal for processing and predicting data. For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition,  machine translation, speech activity detection, robot control, video games, and healthcare.The name of LSTM refers to the analogy that a standard RNN has both "long-term memory" and "short-term memory". The connection weights and biases in the network change once per episode of training, analogous to how physiological changes in synaptic strengths store long-term memories; the activation patterns in the network change once per time-step, analogous to how the moment-to-moment change in electric firing patterns in the brain store short-term memories. The LSTM architecture aims to provide a short-term memory for RNN that can last thousands of timesteps, thus "long short-term memory".A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. Forget gates decide what information to discard from a previous state by assigning a previous state, compared to a current input, a value between 0 and 1. A (rounded) value of 1 means to keep the information, and a value of 0 means to discard it. Input gates decide which pieces of new information to store in the current state, using the same system as forget gates. Output gates control which pieces of information in the current state to output by assigning a value from 0 to 1 to the information, considering the previous and current states. Selectively outputting relevant information from the current state allows the LSTM network to maintain useful, long-term dependencies to make predictions, both in current and future time-steps.
LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications.},
	language = {en},
	urldate = {2023-04-26},
	journal = {Wikipedia},
	month = apr,
	year = {2023},
	note = {Page Version ID: 1148032239},
	file = {Snapshot:/Users/giovanna/Zotero/storage/P4D246Y9/Long_short-term_memory.html:text/html},
}

@misc{noauthor_papers_nodate,
	title = {Papers with {Code} - {BiLSTM} {Explained}},
	url = {https://paperswithcode.com/method/bilstm},
	abstract = {A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence).

Image Source: Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks, Cornegruta et al},
	language = {en},
	urldate = {2023-04-26},
	file = {Snapshot:/Users/giovanna/Zotero/storage/XYTRDEYG/bilstm.html:text/html},
}

@article{hutto_vader_2014,
	title = {{VADER}: {A} {Parsimonious} {Rule}-{Based} {Model} for {Sentiment} {Analysis} of {Social} {Media} {Text}},
	volume = {8},
	copyright = {Copyright (c) 2021 Proceedings of the International AAAI Conference on Web and Social Media},
	issn = {2334-0770},
	shorttitle = {{VADER}},
	url = {https://ojs.aaai.org/index.php/ICWSM/article/view/14550},
	doi = {10.1609/icwsm.v8i1.14550},
	abstract = {The inherent nature of social media content poses serious challenges to practical applications of sentiment analysis. We present VADER, a simple rule-based model for general sentiment analysis, and compare its effectiveness to eleven typical state-of-practice benchmarks including LIWC, ANEW, the General Inquirer, SentiWordNet, and machine learning oriented techniques relying on Naive Bayes, Maximum Entropy, and Support Vector Machine (SVM) algorithms. Using a combination of qualitative and quantitative methods, we first construct and empirically validate a gold-standard list of lexical features (along with their associated sentiment intensity measures) which are specifically attuned to sentiment in microblog-like contexts. We then combine these lexical features with consideration for five general rules that embody grammatical and syntactical conventions for expressing and emphasizing sentiment intensity. Interestingly, using our parsimonious rule-based model to assess the sentiment of tweets, we find that VADER outperforms individual human raters (F1 Classification Accuracy = 0.96 and 0.84, respectively), and generalizes more favorably across contexts than any of our benchmarks.},
	language = {en},
	number = {1},
	urldate = {2023-04-26},
	journal = {Proceedings of the International AAAI Conference on Web and Social Media},
	author = {Hutto, C. and Gilbert, Eric},
	month = may,
	year = {2014},
	note = {Number: 1},
	keywords = {Human Centered Computing},
	pages = {216--225},
	file = {Full Text PDF:/Users/giovanna/Zotero/storage/2V42MI9Y/Hutto and Gilbert - 2014 - VADER A Parsimonious Rule-Based Model for Sentime.pdf:application/pdf},
}

@inproceedings{gamback_using_2017,
	address = {Vancouver, BC, Canada},
	title = {Using {Convolutional} {Neural} {Networks} to {Classify} {Hate}-{Speech}},
	url = {https://aclanthology.org/W17-3013},
	doi = {10.18653/v1/W17-3013},
	abstract = {The paper introduces a deep learning-based Twitter hate-speech text classification system. The classifier assigns each tweet to one of four predefined categories: racism, sexism, both (racism and sexism) and non-hate-speech. Four Convolutional Neural Network models were trained on resp. character 4-grams, word vectors based on semantic information built using word2vec, randomly generated word vectors, and word vectors combined with character n-grams. The feature set was down-sized in the networks by max-pooling, and a softmax function used to classify tweets. Tested by 10-fold cross-validation, the model based on word2vec embeddings performed best, with higher precision than recall, and a 78.3\% F-score.},
	urldate = {2023-04-26},
	booktitle = {Proceedings of the {First} {Workshop} on {Abusive} {Language} {Online}},
	publisher = {Association for Computational Linguistics},
	author = {Gambäck, Björn and Sikdar, Utpal Kumar},
	month = aug,
	year = {2017},
	pages = {85--90},
	file = {Full Text PDF:/Users/giovanna/Zotero/storage/9YXN3CHN/Gambäck and Sikdar - 2017 - Using Convolutional Neural Networks to Classify Ha.pdf:application/pdf},
}

@misc{kobs_emote-controlled_2023,
	title = {Emote-{Controlled}},
	url = {https://github.com/konstantinkobs/emote-controlled},
	abstract = {Code and data for "Emote-Controlled: Obtaining Implicit Viewer Feedback through Emote based Sentiment Analysis on Comments of Popular Twitch.tv Channels"},
	urldate = {2023-04-26},
	author = {Kobs, Konstantin},
	month = jan,
	year = {2023},
	note = {original-date: 2019-10-08T07:24:49Z},
}

@misc{junior_league_2020,
	address = {Florianópolis, SC, Brazil},
	title = {League of {Legends} and hate speech: a corpus for comments in {Twitch}.tv},
	shorttitle = {League of {Legends} and hate speech},
	url = {https://zenodo.org/record/3735091},
	doi = {10.5281/zenodo.3735091},
	abstract = {League of Legends (LOL) is the most popular game on PC, drawing 8 million concurrent players. A common activity of gamers, besides playing games, is to watch other players presenting tips and tricks. Streaming platforms allow some players to show gameplays and live games. Twitch.tv is the world´s leading live streaming platform.  Considering that hate speech is a ubiquitous problem in online gaming, we collected  985,766 comments from five videos of the top 10  LOL streamers in Twitch.tv platform.  The dataset is freely available in a single file, ensembling all videos/players; and divided by players as well.  These comments are a rich data source for opinion mining, sentiment analysis, topic modeling, and hate speech detection (including sexism and racism).},
	language = {eng},
	urldate = {2023-04-26},
	publisher = {Zenodo},
	author = {Junior, Luiz C. C. Lima and Rodrigues, Lucas D. F. and Junior, Antonio F. L. Jacob and Lobato, Fábio M. F.},
	month = mar,
	year = {2020},
	keywords = {games, hate speech, league of legends, twitch},
	file = {Zenodo Snapshot:/Users/giovanna/Zotero/storage/ASM928C6/3735091.html:text/html},
}

@misc{noauthor_recurrent_2023,
	title = {Recurrent neural network},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Recurrent_neural_network&oldid=1148729712},
	abstract = {A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes can create a cycle, allowing output from some nodes to affect subsequent input to the same nodes. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.The term "recurrent neural network" is used to refer to the class of networks with an infinite impulse response, whereas "convolutional neural network" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.
Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).},
	language = {en},
	urldate = {2023-04-26},
	journal = {Wikipedia},
	month = apr,
	year = {2023},
	note = {Page Version ID: 1148729712},
	file = {Snapshot:/Users/giovanna/Zotero/storage/9TBAZGH7/Recurrent_neural_network.html:text/html},
}

@misc{noauthor_long_2023-1,
	title = {Long short-term memory},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Long_short-term_memory&oldid=1148032239},
	abstract = {Long short-term memory (LSTM) is an artificial neural network used in the fields of artificial intelligence and deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. Such a recurrent neural network (RNN) can process not only single data points (such as images), but also entire sequences of data (such as speech or video). This characteristic makes LSTM networks ideal for processing and predicting data. For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition,  machine translation, speech activity detection, robot control, video games, and healthcare.The name of LSTM refers to the analogy that a standard RNN has both "long-term memory" and "short-term memory". The connection weights and biases in the network change once per episode of training, analogous to how physiological changes in synaptic strengths store long-term memories; the activation patterns in the network change once per time-step, analogous to how the moment-to-moment change in electric firing patterns in the brain store short-term memories. The LSTM architecture aims to provide a short-term memory for RNN that can last thousands of timesteps, thus "long short-term memory".A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. Forget gates decide what information to discard from a previous state by assigning a previous state, compared to a current input, a value between 0 and 1. A (rounded) value of 1 means to keep the information, and a value of 0 means to discard it. Input gates decide which pieces of new information to store in the current state, using the same system as forget gates. Output gates control which pieces of information in the current state to output by assigning a value from 0 to 1 to the information, considering the previous and current states. Selectively outputting relevant information from the current state allows the LSTM network to maintain useful, long-term dependencies to make predictions, both in current and future time-steps.
LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications.},
	language = {en},
	urldate = {2023-04-26},
	journal = {Wikipedia},
	month = apr,
	year = {2023},
	note = {Page Version ID: 1148032239},
	file = {Snapshot:/Users/giovanna/Zotero/storage/LGR6GZFI/Long_short-term_memory.html:text/html},
}

@misc{noauthor_papers_nodate-1,
	title = {Papers with {Code} - {BiLSTM} {Explained}},
	url = {https://paperswithcode.com/method/bilstm},
	abstract = {A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence).

Image Source: Modelling Radiological Language with Bidirectional Long Short-Term Memory Networks, Cornegruta et al},
	language = {en},
	urldate = {2023-04-26},
	file = {Snapshot:/Users/giovanna/Zotero/storage/A92IQVQD/bilstm.html:text/html},
}

@misc{kobs_emote-controlled_2023-1,
	title = {Emote-{Controlled}},
	url = {https://github.com/konstantinkobs/emote-controlled},
	abstract = {Code and data for "Emote-Controlled: Obtaining Implicit Viewer Feedback through Emote based Sentiment Analysis on Comments of Popular Twitch.tv Channels"},
	urldate = {2023-04-26},
	author = {Kobs, Konstantin},
	month = jan,
	year = {2023},
	note = {original-date: 2019-10-08T07:24:49Z},
}

@misc{noauthor_tfkeraslossescategoricalcrossentropy_nodate,
	title = {tf.keras.losses.{CategoricalCrossentropy} {\textbar} {TensorFlow} v2.12.0},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy},
	abstract = {Computes the crossentropy loss between the labels and predictions.},
	language = {en},
	urldate = {2023-04-26},
	journal = {TensorFlow},
	file = {Snapshot:/Users/giovanna/Zotero/storage/6GKUJ83J/CategoricalCrossentropy.html:text/html},
}
